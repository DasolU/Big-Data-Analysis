---
layout: post
title: Few-shot Learning 딥러닝 시 데이터 수가 매우 적을 때?
subititle: 카카오브레인 AutoLearn 연구팀의 퓨샷 러닝 연구
categories: AI
tags: [Deep Learning]
---
## Reference

이 글은 [카카오브레인 AutoLearn 연구팀의 퓨샷 러닝 연구][1]를 참고 요약한 것입니다.

### 논문

https://arxiv.org/pdf/1905.01436.pdf @ CVPR 2019

## EGNN

Edge-Labeling Graph Neural Network for Few-shot Learning

### 필요성

* 딥러닝 시 데이터 수가 매우 적을 때

* {러시안블루, 페르시안, 먼치킨}처럼 고양이의 종류를 구분하는 태스크를 풀 때
  * 같은 범주의 데이터를 더 가깝게, 다른 범주의 데이터를 더 멀게 할 정도로 충분히 복잡하지 않을 때

### 방법

**데이터셋**을 **훈련**에 사용하는 **서포트 데이터(support data)**와 **테스트**에 사용하는 **쿼리 데이터(query data)**로 구성

#### 퓨샷 러닝 태스크: ’**N-way K-shot 문제**'

* N은 범주의 수, K는 범주별 서포트 데이터의 수
* 퓨샷 러닝은 이 K가 매우 작은 상황에서의 모델 학습을 가리킵니다
* 퓨샷 러닝 모델의 성능은 N과 반비례하며 K와는 비례하는 관계
  * K가 많을수록 이 범주에 해당하는 데이터를 예측하는 모델의 성능(추론 정확도)은 높아진다
  * 반면, N의 값이 커질수록 모델 성능은 낮아진다

### 특장점

* 거리 학습과 그래프 신경망의 장점을 합쳤다.

  EGNN은 퓨샷 러닝 문제가 주어졌을 때 쿼리 데이터와 서포트 데이터가 그래프 구조 상에서 상호 작용하며 데이터 간 거리를 계산



## 사이드 프로젝트 적용 포인트

* benchmark 실험: 범주(N)가 10개 이하이며, 서포트 데이터(K)가 1 또는 5개, 쿼리 데이터는 범주당 15개

* 5-way 10-shot 모델의 정확도 80% 수준

## 관련 개념

* Siamese network
* DNN (딥뉴럴넷)
  * CNN(convolutional neural networks)은 대표적인 DNN 응용 알고리즘이다.
    *  CNN은 검증 손실 함수(verification loss function) 값을 최소화할 때까지 훈련
      * 두 입력 데이터의 범주가 같은 상황에서 특징 공간상 거리가 멀면 커집니다.
      * 두 데이터의 범주가 다른 상황에서 거리가 가까워져도 마찬가지고요. 
      * 이런 검증 손실 값을 최소화함으로써 모델은 범주가 같은 두 데이터의 거리가 가까워질수록 또는 범주가 다른 데이터의 거리가 멀어지게 하는 특징을 획득하게 됩니다. 
* GNN 
  * 인공 신경망으로 Dense graph(밀집 그래프) 구조 사용 (백터나 행렬이 아닌)
    * Dense graph(밀집 그래프)란? 모든 노드가 서로 완전히 연결된 것
* Meta learning (메타러닝)
  * 기계 스스로 무엇인가를 배우는 '방법' 또는 '지식'을 습득하는 기계학습 이론

[1]: https://www.kakaobrain.com/blog/106

